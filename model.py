# # -*- coding: utf-8 -*-
# """carprice_prediction.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1aDK4CemfYYkWlOJWa0RYviFS50B5LZ4w
# """

# # Commented out IPython magic to ensure Python compatibility.
# #import required libraries
# import pandas as pd
# import matplotlib.pyplot as plt
# # %matplotlib inline
# import seaborn as sns
# from sklearn.model_selection import train_test_split
# from sklearn.linear_model import LinearRegression
# from sklearn.linear_model import Lasso
# from sklearn import metrics

# car_data = pd.read_csv('D:\FDS MINI PROJECT\MODEL\car_data.csv')

# car_data.head()

# car_data.info()

# car_data.isnull().sum()

# car_data.describe()
# car_data.columns
# print(car_data['Fuel_Type'].value_counts())
# print(car_data['Seller_Type'].value_counts())
# print(car_data['Transmission'].value_counts())
# fuel_type = car_data['Fuel_Type']
# seller_type = car_data['Seller_Type']
# transmission_type = car_data['Transmission']
# selling_price = car_data['Selling_Price']

# from matplotlib import style

# style.use('ggplot')
# fig = plt.figure(figsize=(15,5))
# fig.suptitle('Visualizing categorical data columns')
# plt.subplot(1,3,1)
# plt.bar(fuel_type,selling_price, color='royalblue')
# plt.xlabel("Fuel Type")
# plt.ylabel("Selling Price")
# plt.subplot(1,3,2)
# plt.bar(seller_type, selling_price, color='red')
# plt.xlabel("Seller Type")
# plt.subplot(1,3,3)
# plt.bar(transmission_type, selling_price, color='purple')
# plt.xlabel('Transmission type')
# plt.show()

# fig, axes = plt.subplots(1,3,figsize=(15,5), sharey=True)
# fig.suptitle('Visualizing categorical columns')
# sns.barplot(x=fuel_type, y=selling_price, ax=axes[0])
# sns.barplot(x=seller_type, y=selling_price, ax=axes[1])
# sns.barplot(x=transmission_type, y=selling_price, ax=axes[2])

# petrol_data = car_data.groupby('Fuel_Type').get_group('Petrol')
# petrol_data.describe()

# seller = car_data.groupby('Seller_Type').get_group('Dealer')
# seller.describe()

# car_data.replace({'Fuel_Type':{'Petrol':0, 'Diesel':1, 'CNG':2}}, inplace=True)
# #one hot encoding
# car_data = pd.get_dummies(car_data, columns=['Seller_Type', 'Transmission'], drop_first=True)

# numeric_columns = car_data.select_dtypes(include=['float64', 'int64']).columns
# correlation_data = car_data[numeric_columns]


# plt.figure(figsize=(10,7))
# sns.heatmap(car_data.corr(), annot=True)
# plt.title('Correlation between the columns')
# plt.show()

# fig=plt.figure(figsize=(7,5))
# plt.title('Correlation between present price and selling price')
# sns.regplot(x='Present_Price', y='Selling_Price', data=car_data)

# X = car_data.drop(['Car_Name','Selling_Price'], axis=1)
# y = car_data['Selling_Price']

# print("Shape of X is: ",X.shape)
# print("Shape of y is: ", y.shape)

# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)

# print("X_test shape:", X_test.shape)
# print("X_train shape:", X_train.shape)
# print("y_test shape: ", y_test.shape)
# print("y_train shape:", y_train.shape)

# from sklearn.preprocessing import StandardScaler
# scaler = StandardScaler()

# X_train = scaler.fit_transform(X_train)
# X_test = scaler.transform(X_test)

# model = LinearRegression()

# model.fit(X_train, y_train)

# pred = model.predict(X_test)

# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# print("MAE: ", (metrics.mean_absolute_error(pred, y_test)))
# print("MSE: ", (metrics.mean_squared_error(pred, y_test)))
# print("R2 score: ", (metrics.r2_score(pred, y_test)))

# sns.regplot(x=pred, y=y_test)
# plt.xlabel("Predicted Price")
# plt.ylabel('Actual Price')
# plt.title("ACtual vs predicted price")
# plt.show()

# from sklearn.linear_model import Lasso



# lasso_model = Lasso(alpha=1.0)

# # Fit the model
# lasso_model.fit(X_train, y_train)

# # Make predictions on the test set
# y_pred = lasso_model.predict(X_test)

# # Evaluate the model
# mse = mean_squared_error(y_test, y_pred)
# print(f"Lasso Regression Mean Squared Error: {mse}")

# from sklearn.linear_model import Ridge

# ridge_model = Ridge(alpha=1.0)

# # Fit the model
# ridge_model.fit(X_train, y_train)

# # Make predictions on the test set
# y_pred = ridge_model.predict(X_test)

# # Evaluate the model
# mse = mean_squared_error(y_test, y_pred)
# print(f"Ridge Regression Mean Squared Error: {mse}")

# from sklearn.ensemble import RandomForestRegressor

# rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# # Fit the model
# rf_model.fit(X_train, y_train.ravel())

# # Make predictions on the test set
# y_pred = rf_model.predict(X_test)

# # Evaluate the model
# mse = mean_squared_error(y_test, y_pred)
# print(f"Random Forest Regressor Mean Squared Error: {mse}")

# from sklearn.ensemble import GradientBoostingRegressor

# gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)

# # Fit the model
# gb_model.fit(X_train, y_train.ravel())

# # Make predictions on the test set
# y_pred = gb_model.predict(X_test)

# # Evaluate the model
# mse = mean_squared_error(y_test, y_pred)
# print(f"Gradient Boosting Regressor Mean Squared Error: {mse}")

# from sklearn.model_selection import cross_val_score
# from sklearn.metrics import r2_score
# import pandas as pd


# ridge_model = Ridge(alpha=1.0)
# lasso_model = Lasso(alpha=1.0)
# rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
# gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
# linear_model = LinearRegression()

# models = [ridge_model, lasso_model, rf_model, gb_model, linear_model]
# model_names = ['Ridge', 'Lasso', 'Random Forest', 'Gradient Boosting', 'Linear Regression']

# # Create a DataFrame to store results
# results_df = pd.DataFrame(index=model_names, columns=['R-squared (Train)', 'R-squared (Test)', 'Cross-validated Score'])

# for model, name in zip(models, model_names):
#     # Train the model
#     model.fit(X_train, y_train)

#     # Predict on training set
#     y_train_pred = model.predict(X_train)
#     r2_train = r2_score(y_train, y_train_pred)

#     # Predict on test set
#     y_test_pred = model.predict(X_test)
#     r2_test = r2_score(y_test, y_test_pred)

#     # Cross-validated score
#     cv_scores = cross_val_score(model, X, y.ravel(), cv=5, scoring='r2')

#     # Store results in DataFrame
#     results_df.loc[name] = [r2_train, r2_test, cv_scores.mean()]

# # Display the results
# print(results_df)

# import matplotlib.pyplot as plt

# # Plotting R-squared errors for train and test sets
# fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))

# # Plot R-squared for train set
# results_df[['R-squared (Train)', 'R-squared (Test)']].plot(kind='bar', ax=axes[0], rot=45)
# axes[0].set_title('R-squared for Train and Test Sets')
# axes[0].set_ylabel('R-squared')
# axes[0].set_ylim(0, 1)

# # Plot cross-validated scores
# results_df['Cross-validated Score'].plot(kind='bar', ax=axes[1], rot=45, color='green')
# axes[1].set_title('Cross-validated Scores')
# axes[1].set_ylabel('R-squared')
# axes[1].set_ylim(0, 1)

# plt.tight_layout()
# plt.show()
















# -*- coding: utf-8 -*-
"""carprice_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aDK4CemfYYkWlOJWa0RYviFS50B5LZ4w
"""

# Importing required libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import cross_val_score
import numpy as np

# Load the dataset
car_data = pd.read_csv('D:\FDS MINI PROJECT\MODEL\car_data.csv')

# Display the first few rows of the dataset
print(car_data.head())

# Check the information about the dataset
print(car_data.info())

# Check for missing values
print(car_data.isnull().sum())

# Describe the dataset
print(car_data.describe())

# Visualize categorical data columns
fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)
fig.suptitle('Visualizing categorical columns')
sns.barplot(x='Fuel_Type', y='Selling_Price', data=car_data, ax=axes[0])
sns.barplot(x='Seller_Type', y='Selling_Price', data=car_data, ax=axes[1])
sns.barplot(x='Transmission', y='Selling_Price', data=car_data, ax=axes[2])

# Convert categorical variables to numerical
car_data.replace({'Fuel_Type': {'Petrol': 0, 'Diesel': 1, 'CNG': 2}}, inplace=True)
car_data = pd.get_dummies(car_data, columns=['Seller_Type', 'Transmission'], drop_first=True)

# Visualize correlation between columns
numeric_columns = car_data.select_dtypes(include=['float64', 'int64']).columns
correlation_data = car_data[numeric_columns]

# Visualize correlation between columns
plt.figure(figsize=(10, 7))
sns.heatmap(correlation_data.corr(), annot=True)
plt.title('Correlation between the columns')
plt.show()
# Plot correlation between present price and selling price
plt.figure(figsize=(7, 5))
sns.regplot(x='Present_Price', y='Selling_Price', data=car_data)
plt.title('Correlation between present price and selling price')
plt.show()

# Split the dataset into features and target variable
X = car_data.drop(['Car_Name', 'Selling_Price'], axis=1)
y = car_data['Selling_Price']

# Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Linear Regression
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)
pred_linear = linear_model.predict(X_test)

# Lasso Regression
lasso_model = Lasso(alpha=1.0)
lasso_model.fit(X_train, y_train)
pred_lasso = lasso_model.predict(X_test)

# Ridge Regression
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_train, y_train)
pred_ridge = ridge_model.predict(X_test)

# Random Forest Regression
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
pred_rf = rf_model.predict(X_test)

# Gradient Boosting Regression
gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
gb_model.fit(X_train, y_train)
pred_gb = gb_model.predict(X_test)

# Evaluate models
models = [linear_model, lasso_model, ridge_model, rf_model, gb_model]
model_names = ['Linear Regression', 'Lasso Regression', 'Ridge Regression', 'Random Forest Regression', 'Gradient Boosting Regression']

results_df = pd.DataFrame(index=model_names, columns=['MAE', 'MSE', 'R2 Score'])

for model, name in zip(models, model_names):
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    results_df.loc[name] = [mae, mse, r2]

print(results_df)

# Plotting results
plt.figure(figsize=(10, 5))
results_df[['MAE', 'MSE', 'R2 Score']].plot(kind='bar')
plt.title('Model Evaluation Metrics')
plt.xlabel('Model')
plt.ylabel('Error/Metric Value')
plt.xticks(rotation=45)
plt.legend(loc='upper right')
plt.show()

# Cross-validation
results_cv_df = pd.DataFrame(index=model_names, columns=['Cross-validated Score'])

for model, name in zip(models, model_names):
    cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')
    results_cv_df.loc[name] = [np.mean(cv_scores)]

print(results_cv_df)

# Plot cross-validated scores
plt.figure(figsize=(10, 5))
results_cv_df.plot(kind='bar', color='green')
plt.title('Cross-validated Scores')
plt.xlabel('Model')
plt.ylabel('R-squared')
plt.xticks(rotation=45)
plt.legend(loc='upper right')
plt.show()

import pickle

# Train your model (assuming you have already trained your models)

# For example, let's say you have trained a Linear Regression model named linear_model
# linear_model = LinearRegression()
# linear_model.fit(X_train, y_train)

# After training, you can save the model as a pickle file
with open('model.pkl', 'wb') as f:
    pickle.dump(linear_model, f)
